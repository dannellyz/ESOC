{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESOC Research Document In-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Conversion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents came in the form of \".doc\" files, but in order to utilize the more agile docx2txt python library, a conversion to \".docx\" files was neccessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_location = \"/home/zayne/PycharmProjects/ESOC/all_journals\"\n",
    "def docToDocx(journals_location):\n",
    "    for filename in os.listdir(journals_locations):\n",
    "        if filename.endswith('.doc'):\n",
    "            subprocess.call(['soffice', '--headless', '--convert-to', 'docx', filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once in this format, each one of the newly created \".docx\" files could be read into pyhton and split into the individual journal entries contained within the overall time period journal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allDocxToCsv():\n",
    "    os.chdir(journals_location)\n",
    "    for filename in os.listdir(journals_location):\n",
    "        if filename.endswith('.docx'):\n",
    "            journalEntries(filename)  \n",
    "    print \"All .docx to .csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"journalEntries()\" method utilizes regular exressions to seperate each journal document into the individual entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import docx2txt\n",
    "import re\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "def journalEntries(filename):\n",
    "    print \"{} being converted to csv.\".format(filename)\n",
    "    #Import the docx\n",
    "    journal_all = docx2txt.process(filename)\n",
    "    #Create the regular expression to find journal entires\n",
    "    re_entry = re.compile(\"\\n\\D{0,3}\\d{5}\\n\")\n",
    "    #Collect journal codes\n",
    "    journal_codes = re_entry.findall(journal_all)\n",
    "    #Remove document header\n",
    "    journal_texts = re_entry.split(journal_all)[1:]\n",
    "    #Create Empty Array and add headers\n",
    "    output = [[\"Journal Code\", \"Unified Command\", \"Region\", \"Province\", \"Type of Engagement\", \"Reference\", \"Report RN\", \"Date\", \"Summary Report\", \"Enemy Side\", \"Government Side\", \"Civilian Side\", \"Firearms Gained\", \"Firearms Losses\", \"Items Recovered / Loss\", \"Other Details\", \"Action Taken\"]]\n",
    "    #Remove blanks and outliers\n",
    "    for code, text in zip(journal_codes,journal_texts):\n",
    "        #Split each entry into blocks based on newlines and remove the blanks\n",
    "        blocks = filter(None, text.split(os.linesep))\n",
    "        #Call method to split each entry into csv rows\n",
    "        out = entryToCsv(blocks)\n",
    "        #Add journal code\n",
    "        out[0] = code.strip(\"\\n\")\n",
    "        #Append to overall entry array\n",
    "        output.append(out)\n",
    "    #Turn array into pandas dataframe for ease of export    \n",
    "    df = pd.DataFrame(output)\n",
    "    #Send to csv with same filename, but new extension\n",
    "    df.to_csv(filename.replace(\".docx\",\".csv\"),header=False, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each journal entry follows the basic format decpicted in the below image: <img src=\"ESOC_Journal_Template.png\"> The \"journalEntries()\" method calls the \"entryToCsv()\" method which uses additional regular expressions and pyhton string manipulation to turn each entries set of blocks into a row of a csv with the data organized into the desired columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entryToCsv(blocks):\n",
    "    #Initialize the array to write into a row of the csv\n",
    "    toCsv = [None] * 16\n",
    "    #Data cleaning for some stray parenthesis in the entries\n",
    "    if \")\" in blocks: blocks.remove(\")\")\n",
    "        \n",
    "    #Get Unified Command and Region\n",
    "    commReg = blocks[0].split(\"(\",1)\n",
    "    #Write Unified Command\n",
    "    toCsv[1] = commReg[0]\n",
    "    #Write Region\n",
    "    toCsv[2] = commReg[1].strip(\")\")\n",
    "    \n",
    "    # Get Province and Type of Engagement\n",
    "    provEng = blocks[1].split(\"(\", 1)\n",
    "    # Write Province\n",
    "    toCsv[3] = provEng[0]\n",
    "    # Write Type of Engagement\n",
    "    toCsv[4] = provEng[1].strip(\")\")\n",
    "    \n",
    "    \n",
    "    #Get Reference Details\n",
    "    refDetails = re.split('Ref: |Report RN: |dtd|', blocks[2])[1:]\n",
    "\n",
    "    #Write [Referenc\", Report RN]\n",
    "    toCsv[5:6] = refDetails[:-1]\n",
    "    \n",
    "    #Get and clean date\n",
    "    try:\n",
    "        toCsv[7] = re.search('\\d{2} \\D{3} \\d{4}', refDetails[-1:][0]).group(0)\n",
    "    except:\n",
    "        print (\"Error\", refDetails[-1:][0])\n",
    "        toCsv[7] = \"~\"\n",
    "    #Return if no main report (need to investigate cases)\n",
    "    if len(blocks) < 4:\n",
    "        return toCsv\n",
    "    \n",
    "    #Return if only a main report (need to investigate cases)\n",
    "    #Write Summary Report\n",
    "    toCsv[8] = blocks[3]\n",
    "    if len(blocks) < 5:\n",
    "        return toCsv\n",
    "    \n",
    "    #Dictionary to hold the block numbers for variable sub-headings\n",
    "    detailBlock = {\"Enemy Side:\":0, \"Government Side:\":0, \"Civilian Side:\":0, \"Firearms Gains:\":0, \"Firearms Losses:\":0,\n",
    "                   \"Items Recovered / Loss: \":0, \"Other Details:\":0, \"Action Taken:\":0}\n",
    "\n",
    "    #Main Report Continues before sub-headings\n",
    "    if any(otherDetails not in blocks[4] for otherDetails in detailBlock):\n",
    "        #Continue writing Summary Report\n",
    "        toCsv[8] = toCsv[8] + blocks[4]\n",
    "\n",
    "    #Collect the starting indicies for the present sub-headings\n",
    "    for type in detailBlock:\n",
    "        try:\n",
    "            detailBlock[type] = blocks.index(type)\n",
    "        #Assign -1 for absent sub-headings\n",
    "        except ValueError:\n",
    "            detailBlock[type] = -1\n",
    "    #Sort based on value of sub-heading indicie\n",
    "    sort = sorted(detailBlock.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    #Iterate through sub-headings\n",
    "    for i in range(len(sort)):\n",
    "        #Ignore those that are not present\n",
    "        if sort[i][1] != -1:\n",
    "            #If not at the last present sub-heading collect blocks from the previous sub-heading\n",
    "            if len(sort) - sort.index(sort[i]) != 1:\n",
    "                detailText = \"/\".join(blocks[sort[i][1]+1:sort[i+1][1]])\n",
    "            #Else, last present sub-heading, collect to the end\n",
    "            else:\n",
    "                detailText = \" \".join(blocks[sort[i][1]+1:])\n",
    "            #Index values for the various sub-headings in the row to write\n",
    "            detailIndex = {\"Enemy Side:\": 0, \"Government Side:\": 1, \"Civilian Side:\": 2, \"Firearms Gains:\": 3,\n",
    "                           \"Firearms Losses:\": 4,\n",
    "                           \"Items Recovered / Loss: \": 5, \"Other Details:\": 6, \"Action Taken:\": 7}\n",
    "            #Find the locaiton of the present sub-headings\n",
    "            outIndex = detailIndex[blocks[sort[i][1]]]\n",
    "            #Add to array to write to csv row\n",
    "            toCsv[outIndex + 9] = detailText\n",
    "    #Return array to be written to csv row        \n",
    "    return toCsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this process is complete each \".docx\" docuemtn will have a corresponding \".csv\" file that has its data parsed into the appropriate columns. These are then combined into a single \".csv\" for ease of access and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allCsvtoMaster():\n",
    "    os.chdir(journals_location)\n",
    "    all_data = pd.DataFrame()\n",
    "    for filename in os.listdir(journals_location):\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(filename, header=0)\n",
    "            all_data = all_data.append(df,ignore_index=False)\n",
    "    #Write to a master list\n",
    "    all_data.to_csv(\"ESOC_All_Data_2010_2012_Raw.csv\",header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal-Jan 12.docx being converted to csv.\n",
      "Journal-Jan 10.docx being converted to csv.\n",
      "Journal-Feb 10.docx being converted to csv.\n",
      "Journal-Aug 10.docx being converted to csv.\n",
      "Journal-Sep 10.docx being converted to csv.\n",
      "Journal-Feb 11.docx being converted to csv.\n",
      "Journal-Mar 10.docx being converted to csv.\n",
      "Journal-Jan 11.docx being converted to csv.\n",
      "Journal-Apr 12.docx being converted to csv.\n",
      "Journal-Feb 12.docx being converted to csv.\n",
      "Journal-Nov 12.docx being converted to csv.\n",
      "Journal-Oct 10.docx being converted to csv.\n",
      "Journal-Mar 11.docx being converted to csv.\n",
      "Journal-Nov 11.docx being converted to csv.\n",
      "Journal-May 12.docx being converted to csv.\n",
      "Journal-Dec 12.docx being converted to csv.\n",
      "Journal-Oct 12.docx being converted to csv.\n",
      "Journal-Dec 11.docx being converted to csv.\n",
      "Journal-Mar 12.docx being converted to csv.\n",
      "Journal-Apr 10.docx being converted to csv.\n",
      "Journal-Jun 10.docx being converted to csv.\n",
      "Journal-Jul 11.docx being converted to csv.\n",
      "Journal-Jun 11.docx being converted to csv.\n",
      "Journal-May 11.docx being converted to csv.\n",
      "Journal-Sep 11.docx being converted to csv.\n",
      "Journal-May 10.docx being converted to csv.\n",
      "Journal-Apr 11.docx being converted to csv.\n",
      "Journal-Aug 12.docx being converted to csv.\n",
      "Journal-Jul 10.docx being converted to csv.\n",
      "Journal-Sep 12.docx being converted to csv.\n",
      "Journal-Jun 12.docx being converted to csv.\n",
      "Journal-Nov 10.docx being converted to csv.\n",
      "Journal-Jul 12.docx being converted to csv.\n",
      "Journal-Oct 11.docx being converted to csv.\n",
      "Journal-Aug 11.docx being converted to csv.\n",
      "All .docx to .csv\n"
     ]
    }
   ],
   "source": [
    "allDocxToCsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCsvtoMaster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
